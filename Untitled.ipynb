{ 
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ff215b5",
   "metadata": {},
   "source": [
    "# Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "111ebc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b3ae7a",
   "metadata": {},
   "source": [
    "# Extracting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "349e55fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion</th>\n",
       "      <th>pixels</th>\n",
       "      <th>Usage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>151 150 147 155 148 133 111 140 170 174 182 15...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>231 212 156 164 174 138 161 173 182 200 106 38...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   emotion                                             pixels     Usage\n",
       "0        0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...  Training\n",
       "1        0  151 150 147 155 148 133 111 140 170 174 182 15...  Training\n",
       "2        2  231 212 156 164 174 138 161 173 182 200 106 38...  Training\n",
       "3        4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...  Training\n",
       "4        6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...  Training"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(\"fer2013.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55d7b003",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "emotion     int64\n",
       "pixels     object\n",
       "Usage      object\n",
       "dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adb3f214",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code cell to seperate out training and testing data\n",
    "X_train=[]\n",
    "y_train=[]\n",
    "X_test=[]\n",
    "y_test=[]\n",
    "for index,row in df.iterrows():\n",
    "    p=row['pixels'].split(\" \")\n",
    "    if row['Usage']=='Training':\n",
    "        X_train.append(np.array(p))\n",
    "        y_train.append(row['emotion'])\n",
    "    elif row['Usage']=='PublicTest':\n",
    "        X_test.append(np.array(p))\n",
    "        y_test.append(row['emotion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eed1ed17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conversion from object data type to integer\n",
    "X_train=np.array(X_train,dtype='uint8')\n",
    "y_train=np.array(y_train,dtype='uint8')\n",
    "X_test=np.array(X_test,dtype='uint8')\n",
    "y_test=np.array(y_test,dtype='uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "250efaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reshaping of images\n",
    "X_train=X_train.reshape(X_train.shape[0],48,48,1)\n",
    "X_test=X_test.reshape(X_test.shape[0],48,48,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ace2c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rescaling and formation of multiple images\n",
    "train_datagen=ImageDataGenerator(rescale=1/255,shear_range=0.2,zoom_range=0.2,horizontal_flip=True,rotation_range=5)\n",
    "test_datagen=ImageDataGenerator(rescale=1/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a4a5ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimising data for efficient implementation of model and forming batches\n",
    "train_flow=train_datagen.flow(X_train,y_train,batch_size=64)\n",
    "test_flow=test_datagen.flow(X_test,y_test,batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231fa261",
   "metadata": {},
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74dc6b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn=tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f258f7",
   "metadata": {},
   "source": [
    "Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e3f823d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters=64,kernel_size=3,activation='relu',input_shape=[48,48,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9f7fb2",
   "metadata": {},
   "source": [
    "Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce72b04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2,strides=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ecd882",
   "metadata": {},
   "source": [
    "Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86727b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters=64,kernel_size=3,activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb68c9ea",
   "metadata": {},
   "source": [
    "Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8cf320e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2,strides=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f3f5a8",
   "metadata": {},
   "source": [
    "Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da462cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters=64,kernel_size=3,activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938e8542",
   "metadata": {},
   "source": [
    "Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa130a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2,strides=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76e4211",
   "metadata": {},
   "source": [
    "Flattening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5eece069",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee054fad",
   "metadata": {},
   "source": [
    "Full connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3992bbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units=64,activation='relu'))\n",
    "cnn.add(tf.keras.layers.Dense(units=128,activation='relu'))\n",
    "cnn.add(tf.keras.layers.Dense(units=256,activation='relu'))\n",
    "cnn.add(tf.keras.layers.Dense(units=512,activation='relu'))\n",
    "cnn.add(tf.keras.layers.Dense(units=1024,activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebe0b24",
   "metadata": {},
   "source": [
    "Output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eda02a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units=7,activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7006599",
   "metadata": {},
   "source": [
    "Fitting of Model to data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f8b3df",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/75\n",
      "449/448 [==============================] - 286s 638ms/step - loss: 1.2592 - accuracy: 0.5368 - val_loss: 1.2618 - val_accuracy: 0.5325\n",
      "Epoch 2/75\n",
      "449/448 [==============================] - 293s 653ms/step - loss: 1.2133 - accuracy: 0.5478 - val_loss: 1.2339 - val_accuracy: 0.5442\n",
      "Epoch 3/75\n",
      "449/448 [==============================] - 295s 657ms/step - loss: 1.1942 - accuracy: 0.5501 - val_loss: 1.2603 - val_accuracy: 0.5464\n",
      "Epoch 4/75\n",
      "449/448 [==============================] - 295s 657ms/step - loss: 1.1909 - accuracy: 0.5538 - val_loss: 1.2099 - val_accuracy: 0.5456\n",
      "Epoch 5/75\n",
      "449/448 [==============================] - 296s 660ms/step - loss: 1.1892 - accuracy: 0.5546 - val_loss: 1.2281 - val_accuracy: 0.5436\n",
      "Epoch 6/75\n",
      "449/448 [==============================] - 297s 661ms/step - loss: 1.1754 - accuracy: 0.5568 - val_loss: 1.2340 - val_accuracy: 0.5456\n",
      "Epoch 7/75\n",
      "449/448 [==============================] - 303s 675ms/step - loss: 1.1768 - accuracy: 0.5571 - val_loss: 1.2287 - val_accuracy: 0.5525\n",
      "Epoch 8/75\n",
      "449/448 [==============================] - 5029s 11s/step - loss: 1.1646 - accuracy: 0.5577 - val_loss: 1.2110 - val_accuracy: 0.5589\n",
      "Epoch 9/75\n",
      "449/448 [==============================] - 4845s 11s/step - loss: 1.1626 - accuracy: 0.5637 - val_loss: 1.2506 - val_accuracy: 0.5525\n",
      "Epoch 10/75\n",
      "449/448 [==============================] - 4370s 10s/step - loss: 1.1524 - accuracy: 0.5661 - val_loss: 1.2181 - val_accuracy: 0.5472\n",
      "Epoch 11/75\n",
      "449/448 [==============================] - 1471s 3s/step - loss: 1.1531 - accuracy: 0.5668 - val_loss: 1.2031 - val_accuracy: 0.5587\n",
      "Epoch 12/75\n",
      "449/448 [==============================] - 1716s 4s/step - loss: 1.1411 - accuracy: 0.5722 - val_loss: 1.2126 - val_accuracy: 0.5626\n",
      "Epoch 13/75\n",
      "449/448 [==============================] - 2490s 6s/step - loss: 1.1432 - accuracy: 0.5676 - val_loss: 1.2095 - val_accuracy: 0.5584\n",
      "Epoch 14/75\n",
      "449/448 [==============================] - 2648s 6s/step - loss: 1.1376 - accuracy: 0.5719 - val_loss: 1.2486 - val_accuracy: 0.5603\n",
      "Epoch 15/75\n",
      "449/448 [==============================] - 2051s 5s/step - loss: 1.1462 - accuracy: 0.5669 - val_loss: 1.1887 - val_accuracy: 0.5542\n",
      "Epoch 16/75\n",
      "449/448 [==============================] - 290s 646ms/step - loss: 1.1268 - accuracy: 0.5747 - val_loss: 1.2040 - val_accuracy: 0.5556\n",
      "Epoch 17/75\n",
      "449/448 [==============================] - 293s 652ms/step - loss: 1.1306 - accuracy: 0.5726 - val_loss: 1.1997 - val_accuracy: 0.5578\n",
      "Epoch 18/75\n",
      "449/448 [==============================] - 301s 670ms/step - loss: 1.1252 - accuracy: 0.5753 - val_loss: 1.2605 - val_accuracy: 0.5628\n",
      "Epoch 19/75\n",
      "449/448 [==============================] - 299s 665ms/step - loss: 1.1190 - accuracy: 0.5770 - val_loss: 1.2030 - val_accuracy: 0.5623\n",
      "Epoch 20/75\n",
      "449/448 [==============================] - 776s 2s/step - loss: 1.1200 - accuracy: 0.5794 - val_loss: 1.2388 - val_accuracy: 0.5561\n",
      "Epoch 21/75\n",
      "449/448 [==============================] - 287s 639ms/step - loss: 1.1165 - accuracy: 0.5807 - val_loss: 1.2343 - val_accuracy: 0.5614\n",
      "Epoch 22/75\n",
      "449/448 [==============================] - 291s 648ms/step - loss: 1.1181 - accuracy: 0.5785 - val_loss: 1.2162 - val_accuracy: 0.5715\n",
      "Epoch 23/75\n",
      "449/448 [==============================] - 295s 656ms/step - loss: 1.1044 - accuracy: 0.5828 - val_loss: 1.1807 - val_accuracy: 0.5653\n",
      "Epoch 24/75\n",
      "449/448 [==============================] - 294s 655ms/step - loss: 1.1035 - accuracy: 0.5842 - val_loss: 1.1757 - val_accuracy: 0.5704\n",
      "Epoch 25/75\n",
      "449/448 [==============================] - 462s 1s/step - loss: 1.0980 - accuracy: 0.5858 - val_loss: 1.1643 - val_accuracy: 0.5648\n",
      "Epoch 26/75\n",
      "449/448 [==============================] - 552s 1s/step - loss: 1.0960 - accuracy: 0.5866 - val_loss: 1.1637 - val_accuracy: 0.5642\n",
      "Epoch 27/75\n",
      "449/448 [==============================] - 293s 651ms/step - loss: 1.0937 - accuracy: 0.5885 - val_loss: 1.1486 - val_accuracy: 0.5882\n",
      "Epoch 28/75\n",
      "449/448 [==============================] - 296s 660ms/step - loss: 1.0935 - accuracy: 0.5854 - val_loss: 1.1553 - val_accuracy: 0.5773\n",
      "Epoch 29/75\n",
      "449/448 [==============================] - 300s 669ms/step - loss: 1.0846 - accuracy: 0.5944 - val_loss: 1.1796 - val_accuracy: 0.5617\n",
      "Epoch 30/75\n",
      "449/448 [==============================] - 301s 670ms/step - loss: 1.0851 - accuracy: 0.5942 - val_loss: 1.1827 - val_accuracy: 0.5648\n",
      "Epoch 31/75\n",
      "449/448 [==============================] - 2974s 7s/step - loss: 1.0818 - accuracy: 0.5909 - val_loss: 1.1872 - val_accuracy: 0.5678\n",
      "Epoch 32/75\n",
      "449/448 [==============================] - 27094s 60s/step - loss: 1.0853 - accuracy: 0.5901 - val_loss: 1.1693 - val_accuracy: 0.5651\n",
      "Epoch 33/75\n",
      "449/448 [==============================] - 13835s 31s/step - loss: 1.0746 - accuracy: 0.5933 - val_loss: 1.1680 - val_accuracy: 0.5759\n",
      "Epoch 34/75\n",
      "449/448 [==============================] - 289s 644ms/step - loss: 1.0762 - accuracy: 0.5931 - val_loss: 1.1934 - val_accuracy: 0.5631\n",
      "Epoch 35/75\n",
      "449/448 [==============================] - 291s 648ms/step - loss: 1.0714 - accuracy: 0.5994 - val_loss: 1.1929 - val_accuracy: 0.5690\n",
      "Epoch 36/75\n",
      "449/448 [==============================] - 339s 755ms/step - loss: 1.0706 - accuracy: 0.5964 - val_loss: 1.1808 - val_accuracy: 0.5720\n",
      "Epoch 37/75\n",
      "449/448 [==============================] - 3852s 9s/step - loss: 1.0653 - accuracy: 0.5984 - val_loss: 1.1842 - val_accuracy: 0.5759\n",
      "Epoch 38/75\n",
      "449/448 [==============================] - 22516s 50s/step - loss: 1.0657 - accuracy: 0.5988 - val_loss: 1.1670 - val_accuracy: 0.5734\n",
      "Epoch 39/75\n",
      "321/448 [====================>.........] - ETA: 1:17 - loss: 1.0558 - accuracy: 0.6012"
     ]
    }
   ],
   "source": [
    "cnn.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "history=cnn.fit_generator(train_flow, \n",
    "                    steps_per_epoch=len(X_train)/64, \n",
    "                    epochs=75,  \n",
    "                    verbose=1,  \n",
    "                    validation_data=test_flow,\n",
    "                    validation_steps=len(X_test)/64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94310a78",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#save model\n",
    "cnn.save('CNN model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52856485",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the model\n",
    "cnn_copy=tf.keras.models.load_model('CNN model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3856aa8",
   "metadata": {},
   "source": [
    "# Real-Time Emotion Detection using Webcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02fd9535",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "face_haar_cascade=cv2.CascadeClassifier(cv2.data.haarcascades+'haarcascade_frontalface_default.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86ad2b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame=cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    _,test_image=frame.read()\n",
    "    converted_image=cv2.cvtColor(test_image,cv2.COLOR_BGR2GRAY)                # Conversion to Black and White of a Frame\n",
    "\n",
    "    faces=face_haar_cascade.detectMultiScale(converted_image,1.3,5)\n",
    "    for (x,y,w,h) in faces:\n",
    "        cv2.rectangle(test_image,(x,y),(x+w,y+h),(255,0,0))                    # Detecting Face in Camera\n",
    "        roi_gray=converted_image[y:y+w,x:x+h]                                  # Marking region of interest for emotion recognition\n",
    "        roi_gray=cv2.resize(roi_gray,(48,48))\n",
    "        image_pixels=np.asarray(roi_gray)                                      # Conversion of image to array of pixels\n",
    "        image_pixels=np.expand_dims(image_pixels,axis=0)\n",
    "        image_pixels=image_pixels/255\n",
    "        image_pixels=image_pixels.reshape(1,48,48,1)                           # Reshaping image_pixels to match input dimensions of the model\n",
    "        predictions=cnn_copy.predict(image_pixels)\n",
    "        \n",
    "        max_index=np.argmax(predictions[0])                                    # Finding index of Emotion with maximum Probability\n",
    "        emotion_detection=('angry','disgust','fear','happy','sad','surprise','neutral')\n",
    "        emotion_prediction=emotion_detection[max_index]\n",
    "\n",
    "        cv2.putText(test_image,emotion_prediction,(int(x),int(y)),cv2.FONT_HERSHEY_SIMPLEX,2,(128,255,0),3)  # Printing Emotion detected on the frame\n",
    "        cv2.imshow('Emotion',test_image)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF==ord('q'):                                        # Exit from Window\n",
    "        break\n",
    "\n",
    "\n",
    "frame.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731f8cf1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
